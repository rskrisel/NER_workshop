{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rskrisel/NER_workshop/blob/main/NER_workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e45183e1",
      "metadata": {
        "id": "e45183e1"
      },
      "source": [
        "# Named Entity Recognition\n",
        "\n",
        "In this workshop, we are going to learn how to transform large amounts of text into a database using Named Entity Recognition (NER). NER can computationally identify people, places, laws, events, dates, and other elements in a text or collection of texts.\n",
        "\n",
        "## What is Named Entity Recognition?\n",
        "*Explanation borrowed from Melanie Walsh's [Introduction to Cultural Analytics & Python](https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/12-Named-Entity-Recognition.html)*\n",
        "</br>\n",
        "</br>\n",
        "Named Entity Recognition is a fundamental task in the field of natural language processing (NLP). NLP is an interdisciplinary field that blends linguistics, statistics, and computer science. The heart of NLP is to understand human language with statistics and computers. Applications of NLP are all around us. Have you ever heard of a little thing called spellcheck? How about autocomplete, Google translate, chat bots, or Siri? These are all examples of NLP in action!\n",
        "\n",
        "Thanks to recent advances in machine learning and to increasing amounts of available text data on the web, NLP has grown by leaps and bounds in the last decade. NLP models that generate texts and images are now getting eerily good.\n",
        "\n",
        "Open-source NLP tools are getting very good, too. We’re going to use one of these open-source tools, the Python library spaCy, for our Named Entity Recognition tasks in this lesson.\n",
        "\n",
        "## What is spaCy?\n",
        "In this workshop, we are using the spaCy library to run the NER. SpaCy relies on machine learning models that were trained on a large amount of carefully-labeled texts. These texts were, in fact, often labeled and corrected by hand. The English-language spaCy model that we’re going to use in this lesson was trained on an annotated corpus called “OntoNotes”: 2 million+ words drawn from “news, broadcast, talk shows, weblogs, usenet newsgroups, and conversational telephone speech,” which were meticulously tagged by a group of researchers and professionals for people’s names and places, for nouns and verbs, for subjects and objects, and much more. Like a lot of other major machine learning projects, OntoNotes was also sponsored by the Defense Advaced Research Projects Agency (DARPA), the branch of the Defense Department that develops technology for the U.S. military.\n",
        "\n",
        "When spaCy identifies people and places in a text or collection of text, the NLP model is actually making predictions about the text based on what it has learned about how people and places function in English-language sentences.\n",
        "\n",
        "### spaCy Named Entities\n",
        "Below is a Named Entities chart for English-language spaCy taken from [its website](https://spacy.io/api/annotation#named-entities). This chart shows the different named entities that spaCy can identify as well as their corresponding type labels.\n",
        "\n",
        "|Type Label|Description|\n",
        "|:---:|:---:|\n",
        "|PERSON|People, including fictional.|\n",
        "|NORP|Nationalities or religious or political groups.|\n",
        "|FAC|Buildings, airports, highways, bridges, etc.|\n",
        "|ORG|Companies, agencies, institutions, etc.|\n",
        "|GPE|Countries, cities, states.|\n",
        "|LOC|Non-GPE locations, mountain ranges, bodies of water.|\n",
        "|PRODUCT|Objects, vehicles, foods, etc. (Not services.)|\n",
        "|EVENT|Named hurricanes, battles, wars, sports events, etc.|\n",
        "|WORK_OF_ART|Titles of books, songs, etc.|\n",
        "|LAW|Named documents made into laws.|\n",
        "|LANGUAGE|Any named language.|\n",
        "|DATE|Absolute or relative dates or periods.|\n",
        "|TIME|Times smaller than a day.|\n",
        "|PERCENT|Percentage, including ”%“.|\n",
        "|MONEY|Monetary values, including unit.|\n",
        "|QUANTITY|Measurements, as of weight or distance.|\n",
        "|ORDINAL|“first”, “second”, etc.|\n",
        "|CARDINAL|Numerals that do not fall under another type.|\n",
        "\n",
        "\n",
        "### Install spaCy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c165597",
      "metadata": {
        "id": "1c165597"
      },
      "outputs": [],
      "source": [
        "# !pip install -U spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa6015f9",
      "metadata": {
        "id": "aa6015f9"
      },
      "source": [
        "### Download the spaCy Language Model\n",
        "Next we need to download the English-language model (en_core_web_sm), which will be processing and making predictions about our texts. This is the model that was trained on the annotated “OntoNotes” corpus. You can download the en_core_web_sm model by running the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fad90088",
      "metadata": {
        "id": "fad90088"
      },
      "outputs": [],
      "source": [
        "# !python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "553417c6",
      "metadata": {
        "id": "553417c6"
      },
      "source": [
        "*Note: spaCy offers models for other languages including Chinese, German, French, Spanish, Portuguese, Russian, Italian, Dutch, Greek, Norwegian, and Lithuanian.*\n",
        "\n",
        "*spaCy offers language and tokenization support for other language via external dependencies — such as PyviKonlpy for Korean*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfafab04",
      "metadata": {
        "id": "bfafab04"
      },
      "source": [
        "## Import all relevant libraries for collecting data and processing the NER"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa9626a5",
      "metadata": {
        "id": "aa9626a5"
      },
      "source": [
        "We will import:\n",
        "- Spacy and displacy to run the NER and visualize our results\n",
        "- en_core_web_sm to import the spaCy language model\n",
        "- Pandas library for organizing and displaying data (we’re also changing the pandas default max row and column width display setting)\n",
        "- Glob and pathlib to connect to folders on our operating system\n",
        "- Requests to get data from an API and also to web scrape\n",
        "- PPrint to make our JSON results readable\n",
        "- Beautiful Soup to make our HTML results readable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba795f95",
      "metadata": {
        "id": "ba795f95"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "import en_core_web_sm\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c13b4269",
      "metadata": {
        "id": "c13b4269"
      },
      "source": [
        "## Load the spaCy language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90cd92e7",
      "metadata": {
        "id": "90cd92e7"
      },
      "outputs": [],
      "source": [
        "nlp = en_core_web_sm.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `en_core_web_sm` model is a small, general-purpose English model that includes parts of speech, dependencies, and named entities.\n",
        "\n",
        "### Comparison of SpaCy's Small, Medium, and Large Models\n",
        "\n",
        "SpaCy offers different English models in small, medium, and large sizes (e.g., `en_core_web_sm`, `en_core_web_md`, and `en_core_web_lg`). These models vary in size, accuracy, and features. Here’s a breakdown of their differences:\n",
        "\n",
        "| Aspect             | Small (`en_core_web_sm`)                    | Medium (`en_core_web_md`)                  | Large (`en_core_web_lg`)                   |\n",
        "|--------------------|---------------------------------------------|--------------------------------------------|--------------------------------------------|\n",
        "| **Size & Speed**   | Smallest and fastest. Low memory usage, suitable for quick processing. | Balanced size and speed. Slower than small, but more accurate. | Largest and slowest. Requires high memory, best for nuanced analysis. |\n",
        "| **Word Vectors**   | Limited or no word vectors. Basic similarity tasks only. | Includes more extensive word vectors. Better for similarity comparisons. | Most extensive word vectors. Best for capturing semantic relationships. |\n",
        "| **Accuracy**       | Basic accuracy for part-of-speech tagging, dependencies, and named entity recognition. | Improved accuracy in named entity recognition and dependency parsing. | Highest accuracy across all tasks, especially beneficial for deep NLP applications. |\n",
        "| **Use Case**       | Prototyping, applications needing speed, or lightweight NLP tasks. | Most general NLP applications needing a balance of accuracy, memory, and speed. | High-stakes applications where accuracy is critical and resources are ample. |\n",
        "\n",
        "### Recommendations\n",
        "- **Small**: Ideal for prototyping or applications requiring speed over accuracy.\n",
        "- **Medium**: A good balance for most NLP tasks, providing reasonable accuracy without high memory demands.\n",
        "- **Large**: Best for applications that prioritize accuracy and can handle the memory and processing requirements.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RSYSYyY8ILaz"
      },
      "id": "RSYSYyY8ILaz"
    },
    {
      "cell_type": "markdown",
      "id": "155133f9",
      "metadata": {
        "id": "155133f9"
      },
      "source": [
        "## Collect your Data: Combining APIs and Web Scraping"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2db7228b",
      "metadata": {
        "id": "2db7228b"
      },
      "source": [
        "In this workshop, we are going to collect data from news articles in two ways. First, by using connect to the NewsAPI and gathering a collection of URLs related to a specific news topic. Next, by web scraping those URLs to save the articles as text files. For detailed instructions on working with the NewsAPI, please refer to this [\"Working with APIs\" tutorial](https://gist.github.com/rskrisel/4ff9629df9f9d6bf5a638b8ba6c13a68) and for detailed instructions on how to web scrape a list of URLs please refer to the [\"Web Scraping Media URLs in Python\"](https://github.com/rskrisel/web_scraping_workshop) tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27652e49",
      "metadata": {
        "id": "27652e49"
      },
      "source": [
        "### Install the News API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5f3be4e",
      "metadata": {
        "id": "b5f3be4e"
      },
      "outputs": [],
      "source": [
        "# !pip install newsapi-python"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f849ee3e",
      "metadata": {
        "id": "f849ee3e"
      },
      "source": [
        "### Store your secret key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c7b4ad2",
      "metadata": {
        "id": "0c7b4ad2"
      },
      "outputs": [],
      "source": [
        "secret= '123456789'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3f0773a",
      "metadata": {
        "id": "e3f0773a"
      },
      "source": [
        "### Define your endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dea322b",
      "metadata": {
        "id": "2dea322b"
      },
      "outputs": [],
      "source": [
        "url = 'https://newsapi.org/v2/everything?'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e15179b0",
      "metadata": {
        "id": "e15179b0"
      },
      "source": [
        "### Define your query parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c8f2c37",
      "metadata": {
        "id": "6c8f2c37"
      },
      "outputs": [],
      "source": [
        "parameters = {\n",
        "    'q': 'drought',\n",
        "    'searchIn':'title',\n",
        "    'pageSize': 20,\n",
        "    'language' : 'en',\n",
        "    'apiKey': secret\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2bb8ebf",
      "metadata": {
        "id": "f2bb8ebf"
      },
      "source": [
        "### Make your data request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "253183e5",
      "metadata": {
        "id": "253183e5"
      },
      "outputs": [],
      "source": [
        "response = requests.get(url, params=parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc374b48",
      "metadata": {
        "id": "cc374b48"
      },
      "source": [
        "### Visualize your JSON results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "758248bb",
      "metadata": {
        "id": "758248bb"
      },
      "outputs": [],
      "source": [
        "response_json = response.json()\n",
        "pprint.pprint(response_json)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6d735cf",
      "metadata": {
        "id": "a6d735cf"
      },
      "source": [
        "### Check what keys exist in your JSON data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a1d2649",
      "metadata": {
        "id": "5a1d2649"
      },
      "outputs": [],
      "source": [
        "response_json.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8243e74",
      "metadata": {
        "id": "d8243e74"
      },
      "source": [
        "### See the data stored in each key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "476c1231",
      "metadata": {
        "id": "476c1231"
      },
      "outputs": [],
      "source": [
        "print(response_json['status'])\n",
        "print(response_json['totalResults'])\n",
        "print(response_json['articles'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbb9725a",
      "metadata": {
        "id": "cbb9725a"
      },
      "source": [
        "### Check the datatype for each key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99d64aca",
      "metadata": {
        "id": "99d64aca"
      },
      "outputs": [],
      "source": [
        "print(type(response_json['status']))\n",
        "print(type(response_json['totalResults']))\n",
        "print(type(response_json['articles']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaab8d11",
      "metadata": {
        "id": "eaab8d11"
      },
      "source": [
        "### Make sure the list reads as a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b794434c",
      "metadata": {
        "id": "b794434c"
      },
      "outputs": [],
      "source": [
        "type(response_json['articles'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "294acc9e",
      "metadata": {
        "id": "294acc9e"
      },
      "source": [
        "### Convert the JSON key into a Pandas Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0188931e",
      "metadata": {
        "id": "0188931e"
      },
      "outputs": [],
      "source": [
        "df_articles = pd.DataFrame(response_json['articles'])\n",
        "df_articles"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45bd6103",
      "metadata": {
        "id": "45bd6103"
      },
      "source": [
        "### Define a function to web scrape text from the list of URLs in the Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0c86488",
      "metadata": {
        "id": "c0c86488"
      },
      "outputs": [],
      "source": [
        "def scrape_article(url):\n",
        "    response = requests.get(url)\n",
        "    response.encoding = 'utf-8'\n",
        "    html_string = response.text\n",
        "    return html_string"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75f6cb42",
      "metadata": {
        "id": "75f6cb42"
      },
      "source": [
        "### Apply the function to the Dataframe and store the results in a new column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39054edb",
      "metadata": {
        "id": "39054edb"
      },
      "outputs": [],
      "source": [
        "df_articles['scraped_text'] = df_articles['url'].apply(scrape_article)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d3517f0",
      "metadata": {
        "id": "1d3517f0"
      },
      "outputs": [],
      "source": [
        "df_articles"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40b6c876",
      "metadata": {
        "id": "40b6c876"
      },
      "source": [
        "### Use the Beautiful Soup library to make the scraped html text legible and save the output in a new `cleaned_text` column"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column 'cleaned_text' by applying the cleaning function to each row in 'scraped_text'\n",
        "df_articles['cleaned_text'] = df_articles['scraped_text'].apply(lambda text: BeautifulSoup(text, \"html.parser\").get_text())"
      ],
      "metadata": {
        "id": "Ux1uFa5LKD3I"
      },
      "id": "Ux1uFa5LKD3I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_articles[['cleaned_text']]"
      ],
      "metadata": {
        "id": "oCojIyuWKyXH"
      },
      "id": "oCojIyuWKyXH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cb25c146",
      "metadata": {
        "id": "cb25c146"
      },
      "source": [
        "### Let's run the NER across the `cleaned_text` column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3b89b9c",
      "metadata": {
        "id": "f3b89b9c"
      },
      "outputs": [],
      "source": [
        "# Apply the NLP pipeline to each row in the 'cleaned_text' column and store results in 'processed_doc'\n",
        "df_articles['processed_doc'] = df_articles['cleaned_text'].apply(nlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "150dabc0",
      "metadata": {
        "id": "150dabc0"
      },
      "source": [
        "### Let's use displacy to visualize our results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the index of the row you want to visualize\n",
        "row_index = 0  # Change this to the desired row index\n",
        "\n",
        "# Select the NLP processed document at the specified index\n",
        "doc = df_articles['processed_doc'].iloc[row_index]\n",
        "\n",
        "# Render the entities for the selected document\n",
        "displacy.render(doc, style=\"ent\")"
      ],
      "metadata": {
        "id": "ViUP-KlVLwSv"
      },
      "id": "ViUP-KlVLwSv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a3228571",
      "metadata": {
        "id": "a3228571"
      },
      "source": [
        "### Let's see a list of the identified entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f00fa77",
      "metadata": {
        "id": "6f00fa77"
      },
      "outputs": [],
      "source": [
        "doc.ents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0097c6d1",
      "metadata": {
        "id": "0097c6d1"
      },
      "source": [
        "### Let's add the entity label next to each entity:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfe72249",
      "metadata": {
        "id": "bfe72249"
      },
      "outputs": [],
      "source": [
        "for named_entity in doc.ents:\n",
        "    print(named_entity, named_entity.label_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a1efbfc",
      "metadata": {
        "id": "4a1efbfc"
      },
      "source": [
        "### Let's filter the results to see all entities labelled as \"PERSON\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f3b679b",
      "metadata": {
        "id": "9f3b679b"
      },
      "outputs": [],
      "source": [
        "for named_entity in doc.ents:\n",
        "    if named_entity.label_ == \"PERSON\":\n",
        "        print(named_entity)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c373c1bf",
      "metadata": {
        "id": "c373c1bf"
      },
      "source": [
        "### Let's filter the results to see all entities labelled as \"NORP\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4512d2c9",
      "metadata": {
        "id": "4512d2c9"
      },
      "outputs": [],
      "source": [
        "for named_entity in doc.ents:\n",
        "    if named_entity.label_ == \"NORP\":\n",
        "        print(named_entity)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ad739e7",
      "metadata": {
        "id": "9ad739e7"
      },
      "source": [
        "### Let's filter the results to see all entities labelled as \"GPE\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b09f08e8",
      "metadata": {
        "id": "b09f08e8"
      },
      "outputs": [],
      "source": [
        "for named_entity in doc.ents:\n",
        "    if named_entity.label_ == \"GPE\":\n",
        "        print(named_entity)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fefa811",
      "metadata": {
        "id": "2fefa811"
      },
      "source": [
        "### Let's filter the results to see all entities labelled as \"LOC\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81057e7d",
      "metadata": {
        "id": "81057e7d"
      },
      "outputs": [],
      "source": [
        "for named_entity in doc.ents:\n",
        "    if named_entity.label_ == \"LOC\":\n",
        "        print(named_entity)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69da9862",
      "metadata": {
        "id": "69da9862"
      },
      "source": [
        "### Let's filter the results to see all entities labelled as \"FAC\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25b1990f",
      "metadata": {
        "id": "25b1990f"
      },
      "outputs": [],
      "source": [
        "for named_entity in doc.ents:\n",
        "    if named_entity.label_ == \"FAC\":\n",
        "        print(named_entity)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95aafdb4",
      "metadata": {
        "id": "95aafdb4"
      },
      "source": [
        "### Let's filter the results to see all entities labelled as \"ORG\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63ee8d79",
      "metadata": {
        "id": "63ee8d79"
      },
      "outputs": [],
      "source": [
        "for named_entity in doc.ents:\n",
        "    if named_entity.label_ == \"ORG\":\n",
        "        print(named_entity)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d29db1a",
      "metadata": {
        "id": "7d29db1a"
      },
      "source": [
        "### Let's define a function that will entify all the entities in our document and save the output as a dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95ea25a5",
      "metadata": {
        "id": "95ea25a5"
      },
      "outputs": [],
      "source": [
        "entities=[]\n",
        "entity_type = []\n",
        "entity_identified = []\n",
        "for named_entity in doc.ents:\n",
        "    entity_type.append(named_entity.label_)\n",
        "    entity_identified.append(named_entity.text)\n",
        "    entity_dict = {'Entity_type': entity_type, 'Entity_identified': entity_identified}\n",
        "    entities.append(entity_dict)\n",
        "print(entities)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca340104",
      "metadata": {
        "id": "ca340104"
      },
      "source": [
        "### Let's build on this function to run this process across our entire collection of texts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa5f06da",
      "metadata": {
        "id": "fa5f06da"
      },
      "outputs": [],
      "source": [
        "# Initialize a list to store entity data for each article\n",
        "all_entities = []\n",
        "\n",
        "# Iterate over each row in the 'processed_doc' column of df_articles\n",
        "for idx, doc in enumerate(df_articles['processed_doc']):\n",
        "    # Collect entity types and texts for each document\n",
        "    entity_type = [ent.label_ for ent in doc.ents]\n",
        "    entity_identified = [ent.text for ent in doc.ents]\n",
        "\n",
        "    # Create a dictionary with the document index as the identifier\n",
        "    ent_dict = {\n",
        "        'Doc_index': idx,  # Use the row index as an identifier\n",
        "        'Entity_type': entity_type,\n",
        "        'Entity_identified': entity_identified\n",
        "    }\n",
        "\n",
        "    # Append the dictionary to the all_entities list\n",
        "    all_entities.append(ent_dict)\n",
        "\n",
        "# Print the list of dictionaries\n",
        "print(all_entities)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8e971cd",
      "metadata": {
        "id": "e8e971cd"
      },
      "source": [
        "### Let's visualize our results in a Pandas Dataframe sorted by the file name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b613fbca",
      "metadata": {
        "id": "b613fbca"
      },
      "outputs": [],
      "source": [
        "df_NER = pd.DataFrame(all_entities)\n",
        "df_NER = df_NER.sort_values(by='Doc_index', ascending=True)\n",
        "df_NER"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6c16aa3",
      "metadata": {
        "id": "f6c16aa3"
      },
      "source": [
        "### Let's explode our Dataframe so we have just one entity value per row pegged to the file name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1991fa82",
      "metadata": {
        "id": "1991fa82"
      },
      "outputs": [],
      "source": [
        "df_NER = df_NER.set_index(['Doc_index'])\n",
        "df_NER = df_NER.apply(pd.Series.explode).reset_index()\n",
        "df_NER[:25]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a58ee4b",
      "metadata": {
        "id": "6a58ee4b"
      },
      "source": [
        "### Let's filter our results by GPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ab2bb9e",
      "metadata": {
        "id": "2ab2bb9e"
      },
      "outputs": [],
      "source": [
        "df_NER[df_NER['Entity_type'] == 'GPE'][:15]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2508a80",
      "metadata": {
        "id": "e2508a80"
      },
      "source": [
        "### Let's filter our results by LAW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c318f940",
      "metadata": {
        "id": "c318f940"
      },
      "outputs": [],
      "source": [
        "df_NER[df_NER['Entity_type'] == 'LAW'][:15]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddbe4557",
      "metadata": {
        "id": "ddbe4557"
      },
      "source": [
        "### Let's filter our results by Money"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fea966bc",
      "metadata": {
        "id": "fea966bc"
      },
      "outputs": [],
      "source": [
        "df_NER[df_NER['Entity_type'] == 'MONEY'][:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1685825b",
      "metadata": {
        "id": "1685825b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}