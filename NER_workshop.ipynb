{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e45183e1",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "In this workshop, we are going to learn how to transform large amounts of text into a database using Named Entity Recognition (NER). NER can computationally identify people, places, laws, events, dates, and other elements in a text or collection of texts.\n",
    "\n",
    "## What is Named Entity Recognition?\n",
    "*Explanation borrowed from Melanie Walsh's [Introduction to Cultural Analytics & Python](https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/12-Named-Entity-Recognition.html)*\n",
    "</br>\n",
    "</br>\n",
    "Named Entity Recognition is a fundamental task in the field of natural language processing (NLP). NLP is an interdisciplinary field that blends linguistics, statistics, and computer science. The heart of NLP is to understand human language with statistics and computers. Applications of NLP are all around us. Have you ever heard of a little thing called spellcheck? How about autocomplete, Google translate, chat bots, or Siri? These are all examples of NLP in action!\n",
    "\n",
    "Thanks to recent advances in machine learning and to increasing amounts of available text data on the web, NLP has grown by leaps and bounds in the last decade. NLP models that generate texts and images are now getting eerily good.\n",
    "\n",
    "Open-source NLP tools are getting very good, too. We’re going to use one of these open-source tools, the Python library spaCy, for our Named Entity Recognition tasks in this lesson.\n",
    "\n",
    "## What is spaCy?\n",
    "In this workshop, we are using the spaCy library to run the NER. SpaCy relies on machine learning models that were trained on a large amount of carefully-labeled texts. These texts were, in fact, often labeled and corrected by hand. The English-language spaCy model that we’re going to use in this lesson was trained on an annotated corpus called “OntoNotes”: 2 million+ words drawn from “news, broadcast, talk shows, weblogs, usenet newsgroups, and conversational telephone speech,” which were meticulously tagged by a group of researchers and professionals for people’s names and places, for nouns and verbs, for subjects and objects, and much more. Like a lot of other major machine learning projects, OntoNotes was also sponsored by the Defense Advaced Research Projects Agency (DARPA), the branch of the Defense Department that develops technology for the U.S. military.\n",
    "\n",
    "When spaCy identifies people and places in a text or collection of text, the NLP model is actually making predictions about the text based on what it has learned about how people and places function in English-language sentences.\n",
    "\n",
    "### spaCy Named Entities\n",
    "Below is a Named Entities chart for English-language spaCy taken from [its website](https://spacy.io/api/annotation#named-entities). This chart shows the different named entities that spaCy can identify as well as their corresponding type labels.\n",
    "\n",
    "|Type Label|Description|\n",
    "|:---:|:---:|\n",
    "|PERSON|People, including fictional.|\n",
    "|NORP|Nationalities or religious or political groups.|\n",
    "|FAC|Buildings, airports, highways, bridges, etc.|\n",
    "|ORG|Companies, agencies, institutions, etc.|\n",
    "|GPE|Countries, cities, states.|\n",
    "|LOC|Non-GPE locations, mountain ranges, bodies of water.|\n",
    "|PRODUCT|Objects, vehicles, foods, etc. (Not services.)|\n",
    "|EVENT|Named hurricanes, battles, wars, sports events, etc.|\n",
    "|WORK_OF_ART|Titles of books, songs, etc.|\n",
    "|LAW|Named documents made into laws.|\n",
    "|LANGUAGE|Any named language.|\n",
    "|DATE|Absolute or relative dates or periods.|\n",
    "|TIME|Times smaller than a day.|\n",
    "|PERCENT|Percentage, including ”%“.|\n",
    "|MONEY|Monetary values, including unit.|\n",
    "|QUANTITY|Measurements, as of weight or distance.|\n",
    "|ORDINAL|“first”, “second”, etc.|\n",
    "|CARDINAL|Numerals that do not fall under another type.|\n",
    "\n",
    "\n",
    "### Install spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c165597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6015f9",
   "metadata": {},
   "source": [
    "### Download the spaCy Language Model\n",
    "Next we need to download the English-language model (en_core_web_sm), which will be processing and making predictions about our texts. This is the model that was trained on the annotated “OntoNotes” corpus. You can download the en_core_web_sm model by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad90088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553417c6",
   "metadata": {},
   "source": [
    "*Note: spaCy offers models for other languages including Chinese, German, French, Spanish, Portuguese, Russian, Italian, Dutch, Greek, Norwegian, and Lithuanian.*\n",
    "\n",
    "*spaCy offers language and tokenization support for other language via external dependencies — such as PyviKonlpy for Korean*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfafab04",
   "metadata": {},
   "source": [
    "## Import all relevant libraries for collecting data and processing the NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9626a5",
   "metadata": {},
   "source": [
    "We will import:\n",
    "- Spacy and displacy to run the NER and visualize our results\n",
    "- en_core_web_sm to import the spaCy language model\n",
    "- Pandas library for organizing and displaying data (we’re also changing the pandas default max row and column width display setting)\n",
    "- Glob and pathlib to connect to folders on our operating system\n",
    "- Requests to get data from an API and also to web scrape\n",
    "- PPrint to make our JSON results readable\n",
    "- Beautiful Soup to make our HTML results readable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba795f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 600\n",
    "pd.options.display.max_colwidth = 400\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pprint\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13b4269",
   "metadata": {},
   "source": [
    "## Load the spaCy language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cd92e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155133f9",
   "metadata": {},
   "source": [
    "## Collect your Data: Combining APIs and Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db7228b",
   "metadata": {},
   "source": [
    "In this workshop, we are going to collect data from news articles in two ways. First, by using connect to the NewsAPI and gathering a collection of URLs related to a specific news topic. Next, by web scraping those URLs to save the articles as text files. For detailed instructions on working with the NewsAPI, please refer to this [\"Working with APIs\" tutorial](https://gist.github.com/rskrisel/4ff9629df9f9d6bf5a638b8ba6c13a68) and for detailed instructions on how to web scrape a list of URLs please refer to the [\"Web Scraping Media URLs in Python\"](https://github.com/rskrisel/web_scraping_workshop) tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27652e49",
   "metadata": {},
   "source": [
    "### Install the News API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f3be4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install newsapi-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f849ee3e",
   "metadata": {},
   "source": [
    "### Store your secret key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7b4ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "secret= '123456789'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f0773a",
   "metadata": {},
   "source": [
    "### Define your endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dea322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://newsapi.org/v2/everything?'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15179b0",
   "metadata": {},
   "source": [
    "### Define your query parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f2c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'q': 'CHIPS Act', \n",
    "    'pageSize': 20, \n",
    "    'language' : 'en',\n",
    "    'apiKey': secret \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bb8ebf",
   "metadata": {},
   "source": [
    "### Make your data request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253183e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url, params=parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc374b48",
   "metadata": {},
   "source": [
    "### Visualize your JSON results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758248bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_json = response.json()\n",
    "pprint.pprint(response_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d735cf",
   "metadata": {},
   "source": [
    "### Check what keys exist in your JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1d2649",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_json.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8243e74",
   "metadata": {},
   "source": [
    "### See the data stored in each key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c1231",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_json['status'])\n",
    "print(response_json['totalResults'])\n",
    "print(response_json['articles'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb9725a",
   "metadata": {},
   "source": [
    "### Check the datatype for each key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d64aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(response_json['status']))\n",
    "print(type(response_json['totalResults']))\n",
    "print(type(response_json['articles']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaab8d11",
   "metadata": {},
   "source": [
    "### Make sure the list reads as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b794434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(response_json['articles'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294acc9e",
   "metadata": {},
   "source": [
    "### Convert the JSON key into a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0188931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles = pd.DataFrame(response_json['articles'])\n",
    "df_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bd6103",
   "metadata": {},
   "source": [
    "### Define a function to web scrape text from the list of URLs in the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c86488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article(url):\n",
    "    response = requests.get(url)\n",
    "    response.encoding = 'utf-8'\n",
    "    html_string = response.text\n",
    "    return html_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f6cb42",
   "metadata": {},
   "source": [
    "### Apply the function to the Dataframe and store the results in a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39054edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles['scraped_text'] = df_articles['url'].apply(scrape_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3517f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b6c876",
   "metadata": {},
   "source": [
    "### Use the Beautiful Soup library to make the scraped html text legible and save each article in a text file\n",
    "*Note: make sure you create a folder named \"files\" before running this step*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf9246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 0\n",
    "for text in df_articles['scraped_text']:\n",
    "    soup = BeautifulSoup(text)\n",
    "    article = soup.get_text()\n",
    "    \n",
    "    id += 1\n",
    "    with open(f\"files/{id}.txt\", \"w\") as file:\n",
    "        file.write(str(article))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466a1932",
   "metadata": {},
   "source": [
    "### Use glob to connect to the file directory where your articles are saved and store it in a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f989ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"files\"\n",
    "articles = glob.glob(f\"{directory}/*.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b945401",
   "metadata": {},
   "source": [
    "### Make sure you have data stores in your files variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f78632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb25c146",
   "metadata": {},
   "source": [
    "### Let's run the NER on a single article first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b89b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"files/3.txt\"\n",
    "text = open(filepath, encoding='utf-8').read()\n",
    "document = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150dabc0",
   "metadata": {},
   "source": [
    "### Let's use displacy to visualize our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92b5c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(document, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3228571",
   "metadata": {},
   "source": [
    "### Let's see a list of the identified entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f00fa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "document.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0097c6d1",
   "metadata": {},
   "source": [
    "### Let's add the entity label next to each entity: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe72249",
   "metadata": {},
   "outputs": [],
   "source": [
    "for named_entity in document.ents:\n",
    "    print(named_entity, named_entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1efbfc",
   "metadata": {},
   "source": [
    "### Let's filter the results to see all entities labelled as \"PERSON\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3b679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for named_entity in document.ents:\n",
    "    if named_entity.label_ == \"PERSON\":\n",
    "        print(named_entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c373c1bf",
   "metadata": {},
   "source": [
    "### Let's filter the results to see all entities labelled as \"NORP\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4512d2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for named_entity in document.ents:\n",
    "    if named_entity.label_ == \"NORP\":\n",
    "        print(named_entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad739e7",
   "metadata": {},
   "source": [
    "### Let's filter the results to see all entities labelled as \"GPE\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09f08e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for named_entity in document.ents:\n",
    "    if named_entity.label_ == \"GPE\":\n",
    "        print(named_entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fefa811",
   "metadata": {},
   "source": [
    "### Let's filter the results to see all entities labelled as \"LOC\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81057e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for named_entity in document.ents:\n",
    "    if named_entity.label_ == \"LOC\":\n",
    "        print(named_entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69da9862",
   "metadata": {},
   "source": [
    "### Let's filter the results to see all entities labelled as \"FAC\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b1990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for named_entity in document.ents:\n",
    "    if named_entity.label_ == \"FAC\":\n",
    "        print(named_entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aafdb4",
   "metadata": {},
   "source": [
    "### Let's filter the results to see all entities labelled as \"ORG\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ee8d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "for named_entity in document.ents:\n",
    "    if named_entity.label_ == \"ORG\":\n",
    "        print(named_entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca340104",
   "metadata": {},
   "source": [
    "### Now, let's define a function that will run this process across our entire collection of texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5f06da",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entities = []\n",
    "for filepath in articles:\n",
    "    text = open(filepath, encoding='utf-8').read()\n",
    "    doc = nlp(text)\n",
    "    entity_type = [] \n",
    "    for ent in doc.ents:\n",
    "        entity_type.append(ent.label_)\n",
    "    entity_identified = [] \n",
    "    for ent in doc.ents:\n",
    "        entity_identified.append(ent.text)\n",
    "    ent_dict = {'File_name': filepath, 'Entity_type': entity_type, 'Entity_identified': entity_identified}\n",
    "    all_entities.append(ent_dict)\n",
    "print(all_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e971cd",
   "metadata": {},
   "source": [
    "### Let's visualize our results in a Pandas Dataframe sorted by the file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b613fbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NER = pd.DataFrame(all_entities)\n",
    "df_NER = df_NER.sort_values(by='File_name', ascending=True)\n",
    "df_NER "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c16aa3",
   "metadata": {},
   "source": [
    "### Let's explode our Dataframe so we have just one entity value per row pegged to the file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1991fa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NER = df_NER.set_index(['File_name'])\n",
    "df_NER = df_NER.apply(pd.Series.explode).reset_index()\n",
    "df_NER[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a58ee4b",
   "metadata": {},
   "source": [
    "### Let's filter our results by GPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab2bb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NER[df_NER['Entity_type'] == 'GPE'][:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2508a80",
   "metadata": {},
   "source": [
    "### Let's filter our results by LAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c318f940",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NER[df_NER['Entity_type'] == 'LAW'][:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbe4557",
   "metadata": {},
   "source": [
    "### Let's filter our results by Money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea966bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NER[df_NER['Entity_type'] == 'MONEY'][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1685825b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
